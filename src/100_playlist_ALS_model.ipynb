{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "subset100 = pd.read_csv(\"../raw_data/track_meta_100subset_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-val-test split (20%)\n",
    "train, test = train_test_split(subset100, test_size=0.2, random_state=42, stratify = subset100['Playlistid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Playlistid</th>\n",
       "      <th>Trackid</th>\n",
       "      <th>Artist_Name</th>\n",
       "      <th>Track_Name</th>\n",
       "      <th>Album_Name</th>\n",
       "      <th>Track_Duration</th>\n",
       "      <th>Artist_uri</th>\n",
       "      <th>Track_uri</th>\n",
       "      <th>Album_uri</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>...</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "      <th>Playlist</th>\n",
       "      <th>Album</th>\n",
       "      <th>Track</th>\n",
       "      <th>Artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>38828</td>\n",
       "      <td>35</td>\n",
       "      <td>Bastille</td>\n",
       "      <td>Pompeii</td>\n",
       "      <td>Bad Blood</td>\n",
       "      <td>214147</td>\n",
       "      <td>spotify:artist:7EQ0qTo7fWT7DPxmxtSYEc</td>\n",
       "      <td>spotify:track:3gbBpTdY8lnQwqxNCcf795</td>\n",
       "      <td>spotify:album:64fQ94AVziavTPdnkCS6Nj</td>\n",
       "      <td>0.0755</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.383</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>127.435</td>\n",
       "      <td>4</td>\n",
       "      <td>0.571</td>\n",
       "      <td>tb</td>\n",
       "      <td>55</td>\n",
       "      <td>63</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>38828</td>\n",
       "      <td>34</td>\n",
       "      <td>Britney Spears</td>\n",
       "      <td>Womanizer</td>\n",
       "      <td>Circus (Deluxe Version)</td>\n",
       "      <td>224400</td>\n",
       "      <td>spotify:artist:26dSoYclwsYLMAKD3tpOr4</td>\n",
       "      <td>spotify:track:4fixebDZAVToLbUCuEloa2</td>\n",
       "      <td>spotify:album:2tve5DGwub1TtbX1khPX5j</td>\n",
       "      <td>0.0730</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.226</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0622</td>\n",
       "      <td>139.000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.235</td>\n",
       "      <td>tb</td>\n",
       "      <td>55</td>\n",
       "      <td>63</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2414</th>\n",
       "      <td>229646</td>\n",
       "      <td>7</td>\n",
       "      <td>Soft Cell</td>\n",
       "      <td>Tainted Love</td>\n",
       "      <td>Non-Stop Erotic Cabaret</td>\n",
       "      <td>153762</td>\n",
       "      <td>spotify:artist:6aq8T2RcspxVOGgMrTzjWc</td>\n",
       "      <td>spotify:track:0cGG2EouYCEEC3xfa0tDFV</td>\n",
       "      <td>spotify:album:3KFWViJ1wIHAdOVLFTVzjD</td>\n",
       "      <td>0.4620</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.284</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>144.435</td>\n",
       "      <td>4</td>\n",
       "      <td>0.623</td>\n",
       "      <td>Throwback</td>\n",
       "      <td>121</td>\n",
       "      <td>135</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>186672</td>\n",
       "      <td>28</td>\n",
       "      <td>Imagine Dragons</td>\n",
       "      <td>Radioactive</td>\n",
       "      <td>Night Visions</td>\n",
       "      <td>186813</td>\n",
       "      <td>spotify:artist:53XhwfbYqKCa1cC15pYq2q</td>\n",
       "      <td>spotify:track:6Ep6BzIOB9tz3P4sWqiiAB</td>\n",
       "      <td>spotify:album:1vAEF8F0HoRFGiYOEeJXHW</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.698</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>136.249</td>\n",
       "      <td>4</td>\n",
       "      <td>0.210</td>\n",
       "      <td>campfire</td>\n",
       "      <td>30</td>\n",
       "      <td>34</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>37634</td>\n",
       "      <td>17</td>\n",
       "      <td>LANY</td>\n",
       "      <td>WHERE THE HELL ARE MY FRIENDS</td>\n",
       "      <td>WHERE THE HELL ARE MY FRIENDS</td>\n",
       "      <td>216180</td>\n",
       "      <td>spotify:artist:49tQo2QULno7gxHutgccqF</td>\n",
       "      <td>spotify:track:4TA2nSix6i8K2VV9wt6rUn</td>\n",
       "      <td>spotify:album:34ySll9UQXpSngEI0NJbFO</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.811</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>127.994</td>\n",
       "      <td>4</td>\n",
       "      <td>0.472</td>\n",
       "      <td>not sure</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Playlistid  Trackid      Artist_Name                     Track_Name  \\\n",
       "557        38828       35         Bastille                        Pompeii   \n",
       "556        38828       34   Britney Spears                      Womanizer   \n",
       "2414      229646        7        Soft Cell                   Tainted Love   \n",
       "1771      186672       28  Imagine Dragons                    Radioactive   \n",
       "516        37634       17             LANY  WHERE THE HELL ARE MY FRIENDS   \n",
       "\n",
       "                         Album_Name  Track_Duration  \\\n",
       "557                       Bad Blood          214147   \n",
       "556         Circus (Deluxe Version)          224400   \n",
       "2414        Non-Stop Erotic Cabaret          153762   \n",
       "1771                  Night Visions          186813   \n",
       "516   WHERE THE HELL ARE MY FRIENDS          216180   \n",
       "\n",
       "                                 Artist_uri  \\\n",
       "557   spotify:artist:7EQ0qTo7fWT7DPxmxtSYEc   \n",
       "556   spotify:artist:26dSoYclwsYLMAKD3tpOr4   \n",
       "2414  spotify:artist:6aq8T2RcspxVOGgMrTzjWc   \n",
       "1771  spotify:artist:53XhwfbYqKCa1cC15pYq2q   \n",
       "516   spotify:artist:49tQo2QULno7gxHutgccqF   \n",
       "\n",
       "                                 Track_uri  \\\n",
       "557   spotify:track:3gbBpTdY8lnQwqxNCcf795   \n",
       "556   spotify:track:4fixebDZAVToLbUCuEloa2   \n",
       "2414  spotify:track:0cGG2EouYCEEC3xfa0tDFV   \n",
       "1771  spotify:track:6Ep6BzIOB9tz3P4sWqiiAB   \n",
       "516   spotify:track:4TA2nSix6i8K2VV9wt6rUn   \n",
       "\n",
       "                                 Album_uri  acousticness   ...   loudness  \\\n",
       "557   spotify:album:64fQ94AVziavTPdnkCS6Nj        0.0755   ...     -6.383   \n",
       "556   spotify:album:2tve5DGwub1TtbX1khPX5j        0.0730   ...     -5.226   \n",
       "2414  spotify:album:3KFWViJ1wIHAdOVLFTVzjD        0.4620   ...     -8.284   \n",
       "1771  spotify:album:1vAEF8F0HoRFGiYOEeJXHW        0.1190   ...     -3.698   \n",
       "516   spotify:album:34ySll9UQXpSngEI0NJbFO        0.0652   ...     -3.811   \n",
       "\n",
       "      mode  speechiness    tempo  time_signature  valence   Playlist  Album  \\\n",
       "557      1       0.0407  127.435               4    0.571         tb     55   \n",
       "556      1       0.0622  139.000               4    0.235         tb     55   \n",
       "2414     0       0.0378  144.435               4    0.623  Throwback    121   \n",
       "1771     1       0.0590  136.249               4    0.210   campfire     30   \n",
       "516      1       0.0344  127.994               4    0.472   not sure     16   \n",
       "\n",
       "      Track  Artist  \n",
       "557      63      44  \n",
       "556      63      44  \n",
       "2414    135      91  \n",
       "1771     34      29  \n",
       "516      23      13  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create co-occurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Binary Sparse Matrix\n",
    "co_mat = pd.crosstab(train.Playlistid, train.Track_uri)\n",
    "co_mat = co_mat.clip(upper=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderSystem():\n",
    "    \"\"\"Represents the scheme for implementing a recommender system.\"\"\"\n",
    "    def __init__(self, training_data, *params):\n",
    "        \"\"\"Initializes the recommender system.\n",
    "\n",
    "        Note that training data has to be provided when instantiating.\n",
    "        Optional parameters are passed to the underlying system.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, *params):\n",
    "        \"\"\"Starts training. Passes optional training parameters to the system.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def score(self, user_id, item_id):\n",
    "        \"\"\"Returns a single score for a user-item pair.\n",
    "\n",
    "        If no prediction for the given pair can be made, an exception should be raised.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ALSRecommenderSystem(RecommenderSystem):\n",
    "    \"\"\"Provides a biased ALS-based implementation of an implicit recommender system.\"\"\"\n",
    "    def __init__(self, training_data, biased, latent_dimension, log_dir=None, confidence=20):\n",
    "        \"\"\"Initializes the recommender system.\n",
    "\n",
    "        Keyword arguments:\n",
    "        training_data: Data to train on.\n",
    "        biased: Whether to include user- and item-related biases in the model.\n",
    "        latent_dimension: Dimension of the latent space.\n",
    "        log_dir: Optional pointer to directory storing logging information.\n",
    "        confidence: Confidence value that should be assigned to pairs where interaction\n",
    "                    was present. Since the data includes single interactions only, simply\n",
    "                    assigining 1 for non-interactions and this value otherwise suffices.\n",
    "                    Should be greater than 1.\n",
    "        \"\"\"\n",
    "        self.biased = biased\n",
    "        self.confidence = confidence\n",
    "        self.latent_dimension = latent_dimension\n",
    "        self.U = None\n",
    "        self.V = None\n",
    "        self.log_dir = log_dir\n",
    "        self.C_users, self.P_users, self.C_items, self.P_items, self.mapping_users, self.mapping_items = self._build_matrices(training_data, confidence)\n",
    "        self.user_dim, self.item_dim = self.P_users.shape\n",
    "\n",
    "    def _build_matrices(self, activity, confidence):\n",
    "        \"\"\"Build the initial matrices.\"\"\"\n",
    "        distinct_users = len(set(activity['user']))\n",
    "        distinct_items = len(set(activity['items']))\n",
    "        C_users = np.ones(shape=(distinct_users, distinct_items))\n",
    "        P_users = np.zeros(shape=(distinct_users, distinct_items))\n",
    "        C_items = np.ones(shape=(distinct_items, distinct_users))\n",
    "        P_items = np.zeros(shape=(distinct_items, distinct_users))\n",
    "\n",
    "        mapping_users = {}\n",
    "        mapping_items = {}\n",
    "        user_ct = 0\n",
    "        items_ct = 0\n",
    "\n",
    "        for index, row in activity.iterrows():\n",
    "            user, items = row\n",
    "            if not user in mapping_users:\n",
    "                mapping_users[user] = user_ct\n",
    "                user_ct += 1\n",
    "            if not items in mapping_items:\n",
    "                mapping_items[items] = items_ct\n",
    "                items_ct += 1\n",
    "            user_index, items_index = mapping_users[user], mapping_items[items]\n",
    "            C_users[user_index, items_index] = confidence\n",
    "            P_users[user_index, items_index] = 1\n",
    "            C_items[items_index, user_index] = confidence\n",
    "            P_items[items_index, user_index] = 1\n",
    "        return C_users, P_users, C_items, P_items, mapping_users, mapping_items\n",
    "\n",
    "    def save(self, directory):\n",
    "        \"\"\"Saves current matrices to the given directory.\"\"\"\n",
    "        np.save(os.path.join(directory, 'U.npy'), self.U)\n",
    "        np.save(os.path.join(directory, 'V.npy'), self.V)\n",
    "        #np.save(os.path.join(directory, 'training_data.npy'), self.training_data)\n",
    "        np.save(os.path.join(directory, 'params.npy'), np.array([self.confidence]))\n",
    "        if self.biased:\n",
    "            np.save(os.path.join(directory, 'user_biases.npy'), self.user_biases)\n",
    "            np.save(os.path.join(directory, 'item_biases.npy'), self.item_biases)\n",
    "\n",
    "    def load(self, directory):\n",
    "        \"\"\"Loads matrices from the given directory.\"\"\"\n",
    "        self.U = np.load(os.path.join(directory, 'U.npy'))\n",
    "        self.V = np.load(os.path.join(directory, 'V.npy'))\n",
    "        self.training_data = np.load(os.path.join(directory, 'training_data.npy'))\n",
    "        self.confidence = np.load(os.path.join(directory, 'params.npy')).flatten()\n",
    "        if self.biased:\n",
    "            self.user_biases = np.load(os.path.join(directory, 'user_biases.npy'))\n",
    "            self.item_biases = np.load(os.path.join(directory, 'item_biases.npy'))\n",
    "\n",
    "        self.C_users, self.P_users, self.C_items, self.P_items, self.mapping_users, self.mapping_items = self._build_matrices(self.training_data, self.confidence)\n",
    "        self.user_dim, self.item_dim = self.P_users.shape\n",
    "\n",
    "    def _single_step(self, lbd):\n",
    "        \"\"\"Executes a single optimization step using (biased) ALS, with lbd as regularization factor.\"\"\"\n",
    "        C_users, P_users, C_items, P_items, mapping_users, mapping_items = self.C_users, self.P_users, self.C_items, self.P_items, self.mapping_users, self.mapping_items\n",
    "        biased = self.biased\n",
    "\n",
    "        # Update U.\n",
    "        if biased: # Expand matrices to account for biases.\n",
    "            U_exp = np.hstack((self.user_biases.reshape(-1,1), self.U))\n",
    "            V_exp = np.hstack((np.ones_like(self.item_biases).reshape(-1,1), self.V))\n",
    "            kdim = self.latent_dimension + 1\n",
    "        else: # We work with copies here to make it safer to abort within updates.\n",
    "            U_exp = self.U.copy()\n",
    "            V_exp = self.V.copy()\n",
    "            kdim = self.latent_dimension\n",
    "        Vt = np.dot(np.transpose(V_exp), V_exp)\n",
    "        for user_index in tqdm(range(self.user_dim)):\n",
    "            C = np.diag(C_users[user_index])\n",
    "            d = np.dot(C, P_users[user_index] - (0 if not biased else self.item_biases))\n",
    "            val = np.dot(np.linalg.inv(Vt + np.dot(np.dot(V_exp.T, C - np.eye(self.item_dim)), V_exp) + lbd*np.eye(kdim)), np.transpose(V_exp))\n",
    "            U_exp[user_index] = np.dot(val, d)\n",
    "        if biased:\n",
    "            self.user_biases = U_exp[:,0]\n",
    "            self.U = U_exp[:,1:]\n",
    "        else:\n",
    "            self.U = U_exp\n",
    "\n",
    "        # Update V.\n",
    "        if biased:\n",
    "            U_exp = np.hstack((np.ones_like(self.user_biases).reshape(-1,1), self.U))\n",
    "            V_exp = np.hstack((self.item_biases.reshape(-1,1), self.V))\n",
    "        else: # We work with copies here to make it safer to abort within updates.\n",
    "            U_exp = self.U.copy()\n",
    "            V_exp = self.V.copy()\n",
    "\n",
    "        Ut = np.dot(np.transpose(U_exp), U_exp)\n",
    "        for item_index in tqdm(range(self.item_dim)):\n",
    "            C = np.diag(C_items[item_index])\n",
    "            d = np.dot(C, P_items[item_index] - (0 if not biased else self.user_biases))\n",
    "            val = np.dot(np.linalg.inv(Ut + np.dot(np.dot(U_exp.T, C-np.eye(self.user_dim)), U_exp) + lbd*np.eye(kdim)), np.transpose(U_exp))\n",
    "            V_exp[item_index] = np.dot(val, d)\n",
    "        if biased:\n",
    "            self.item_biases = V_exp[:, 0]\n",
    "            self.V = V_exp[:,1:]\n",
    "        else:\n",
    "            self.V = V_exp\n",
    "\n",
    "    def compute_loss(self, lbd):\n",
    "        \"\"\"Computes loss value on the training data.\n",
    "\n",
    "        Returns a tuple of total loss and prediction loss (excluding regularization loss).\n",
    "        \"\"\"\n",
    "        C_users, P_users, C_items, P_items, mapping_users, mapping_items = self.C_users, self.P_users, self.C_items, self.P_items, self.mapping_users, self.mapping_items\n",
    "        main_loss = 0\n",
    "        # Main loss term.\n",
    "        for user_index in range(self.user_dim):\n",
    "            for item_index in range(self.item_dim):\n",
    "                pred = np.dot(self.U[user_index].T, self.V[item_index])\n",
    "                if self.biased:\n",
    "                    pred += self.user_biases[user_index] + self.item_biases[item_index]\n",
    "                loss = self.C_users[user_index, item_index] * (P_users[user_index, item_index]-pred)**2\n",
    "                main_loss += loss\n",
    "\n",
    "        # Regularization term.\n",
    "        reg_loss = 0\n",
    "        if lbd > 0:\n",
    "            for user_index in range(self.user_dim):\n",
    "                reg_loss += np.sum(self.U[user_index]**2) + (0 if not self.biased else self.user_biases[user_index]**2)\n",
    "            for item_index in range(self.item_dim):\n",
    "                reg_loss += np.sum(self.V[item_index]**2) + (0 if not self.biased else self.item_biases[item_index]**2)\n",
    "            reg_loss *= lbd\n",
    "        return main_loss + reg_loss, main_loss\n",
    "\n",
    "    def train(self, lbd, iterations=20, verbose=True):\n",
    "        \"\"\"\n",
    "        Trains the recommendation system.\n",
    "\n",
    "        Keyword arguments:\n",
    "        lbd: Regularization factor.\n",
    "        iterations: Number of iterations to run ALS.\n",
    "        verbose: Whether to plot and output training loss.\n",
    "        \"\"\"\n",
    "        if self.U is None or self.V is None:\n",
    "            self.U = np.random.normal(size=(self.user_dim, self.latent_dimension))\n",
    "            self.V = np.random.normal(size=(self.item_dim, self.latent_dimension))\n",
    "            self.user_biases = np.zeros(self.user_dim)\n",
    "            self.item_biases = np.zeros(self.item_dim)\n",
    "            self.history_losses = []\n",
    "            self.history_main_losses = []\n",
    "            self.history_avg_score = []\n",
    "            self.history_avg_rank = []\n",
    "\n",
    "        it = 0\n",
    "        while(it < iterations):\n",
    "            self._single_step(lbd)\n",
    "            loss, main_loss = self.compute_loss(lbd)\n",
    "            self.history_losses.append(loss)\n",
    "            self.history_main_losses.append(main_loss)\n",
    "\n",
    "            if verbose:\n",
    "                clear_output(wait=True)\n",
    "                print('LOSS:', loss, 'MAIN LOSS:', main_loss)\n",
    "\n",
    "                plt.figure(figsize=(5,5))\n",
    "                plt.title('training loss (lower is better)')\n",
    "                plt.plot(range(len(self.history_losses)), self.history_losses)\n",
    "                plt.plot(range(len(self.history_main_losses)), self.history_main_losses, color='orange')\n",
    "                plt.plot(range(len(self.history_main_losses)), np.array(self.history_losses) - np.array(self.history_main_losses), color='green')\n",
    "                plt.legend(['total loss', 'data loss', 'regularizing loss'])\n",
    "                if self.log_dir is not None:\n",
    "                    plt.savefig(os.path.join(self.log_dir, 'log.png'), bbox_inches='tight', format='png')\n",
    "                plt.show()\n",
    "            it += 1\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the recommendation system's internal state.\"\"\"\n",
    "        self.U = None\n",
    "        self.V = None\n",
    "\n",
    "    def score(self, user_id, items_id):\n",
    "        \"\"\"Returns the scoring of item_id for user_id.\"\"\"\n",
    "        if self.U is None or self.V is None:\n",
    "            raise ValueError('system has to be trained first')\n",
    "        if user_id not in self.mapping_users:\n",
    "            raise ValueError('user unknown')\n",
    "        if items_id not in self.mapping_items:\n",
    "            raise ValueError('item unknown')\n",
    "\n",
    "        user_index = self.mapping_users[user_id]\n",
    "        items_index = self.mapping_items[items_id]\n",
    "        pred = np.dot(self.U[user_index], self.V[items_index])\n",
    "        if self.biased: # Include applicable biases.\n",
    "            pred += self.user_biases[user_index] + self.item_biases[items_index]\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>430</td>\n",
       "      <td>spotify:track:0E1NL6gkv5aQKGNjJfBE3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>430</td>\n",
       "      <td>spotify:track:0OuPMjmicFfmnB3SFFqdgQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>430</td>\n",
       "      <td>spotify:track:0TIv1rjOG6Wbc02T4p3y7o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430</td>\n",
       "      <td>spotify:track:1CtOCnWYfIwVgIKiR2Lufw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>430</td>\n",
       "      <td>spotify:track:1lNGwNQX4IrvDwgETwyPjR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user                                 items\n",
       "0  430  spotify:track:0E1NL6gkv5aQKGNjJfBE3A\n",
       "1  430  spotify:track:0OuPMjmicFfmnB3SFFqdgQ\n",
       "2  430  spotify:track:0TIv1rjOG6Wbc02T4p3y7o\n",
       "3  430  spotify:track:1CtOCnWYfIwVgIKiR2Lufw\n",
       "4  430  spotify:track:1lNGwNQX4IrvDwgETwyPjR"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for i, row in co_mat.iterrows():\n",
    "    for track in row[row == 1].index.values:\n",
    "        res.append((i, track))\n",
    "res = pd.DataFrame(np.array(res), columns=['user', 'items'])\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = ALSRecommenderSystem(res, True, latent_dimension=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.train(0.0001, iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_similar_songs_playlist(model, orig_df, target_playlist_id, cand_list_size):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    model: the recommendation system that was trained on the training set with latent factors\n",
    "    orig_df: original df with tracks as rows, but with playlistid and other features (e.g., train)\n",
    "    target_playlist_id: id of the target playlist\n",
    "    target_playlist_inx: index of playlist in the training set\n",
    "    cand_list_size: candidate list of songs to recommend size (= test-set size * 15)\n",
    "    \n",
    "    Output:\n",
    "    k_song_to_recommend: the most similar tracks per track\n",
    "    \"\"\"\n",
    "    target_track_inx = np.where(train[\"Playlistid\"] == target_playlist_id)[0] # index of tracks in training playlist of target playlist\n",
    "    score_allsongs = list(map(lambda x: model.score(str(target_playlist_id), x), orig_df[\"Track_uri\"]))\n",
    "    rec_inx = np.argsort(score_allsongs)[::-1]\n",
    "    \n",
    "    cand_list = orig_df.iloc[rec_inx]['Track_uri']\n",
    "    unique_cand_list = cand_list.drop_duplicates()#list(set(cand_list)) # drop duplciated tracks\n",
    "    \n",
    "    tracks_in_target_playlist = orig_df.loc[orig_df[\"Playlistid\"] == target_playlist_id, \"Track_uri\"]\n",
    "    \n",
    "    cand_list2 = unique_cand_list.loc[~unique_cand_list.isin(tracks_in_target_playlist)] # remove songs that are in the \n",
    "    cand_list3 = cand_list2[:cand_list_size]\n",
    "    return list(cand_list3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nholdout(playlist_id, df):\n",
    "    '''Pass in a playlist id to get number of songs held out in val/test set'''\n",
    "    return len(df[df.Playlistid == playlist_id].Track_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_precision(prediction, val_set):\n",
    "    # prediction should be a list of predictions\n",
    "    # val_set should be pandas Series of ground truths\n",
    "    score = np.sum(val_set.isin(prediction))/val_set.shape[0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NDCG Code Source: https://gist.github.com/bwhite/3726239\n",
    "def dcg_at_k(r, k, method=0):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rps = []\n",
    "ndcgs = []\n",
    "for pid in co_mat.index:\n",
    "    ps = als_similar_songs_playlist(rs, train, pid, nholdout(pid, train)*15)\n",
    "    vs = test[test.Playlistid == pid].Track_uri # ground truth\n",
    "    rps.append(r_precision(ps, vs))\n",
    "    \n",
    "    r = np.zeros(len(ps))\n",
    "    for i, p in enumerate(ps):\n",
    "        if np.any(vs.isin([p])):\n",
    "            r[i] = 1\n",
    "    ndcgs.append(ndcg_at_k(r, len(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. R-Precision:  0.23969273359366242\n",
      "Avg. NDCG:  0.1504029917610824\n",
      "Total Sum:  0.1950478626773724\n"
     ]
    }
   ],
   "source": [
    "avg_rp = np.mean(rps)\n",
    "avg_ndcg = np.mean(ndcgs)\n",
    "print('Avg. R-Precision: ', avg_rp)\n",
    "print('Avg. NDCG: ', avg_ndcg)\n",
    "print('Total Sum: ', np.mean([avg_rp, avg_ndcg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
