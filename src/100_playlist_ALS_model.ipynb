{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "subset27k = pd.read_csv(\"../raw_data/track_meta_milestone3.csv\", index_col=[\"Unnamed: 0\"])\n",
    "# subset100 = pd.read_csv(\"../raw_data/track_meta_100subset_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-val-test split (20%)\n",
    "train, test = train_test_split(subset27k, test_size=0.2, random_state=42, stratify = subset27k['Playlistid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Playlistid</th>\n",
       "      <th>Trackid</th>\n",
       "      <th>Artist_Name</th>\n",
       "      <th>Track_Name</th>\n",
       "      <th>Album_Name</th>\n",
       "      <th>Track_Duration</th>\n",
       "      <th>Artist_uri</th>\n",
       "      <th>Track_uri</th>\n",
       "      <th>Album_uri</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>...</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "      <th>Playlist</th>\n",
       "      <th>Album</th>\n",
       "      <th>Track</th>\n",
       "      <th>Artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7417052</th>\n",
       "      <td>111600</td>\n",
       "      <td>0</td>\n",
       "      <td>YG</td>\n",
       "      <td>Why You Always Hatin?</td>\n",
       "      <td>Still Brazy</td>\n",
       "      <td>196600</td>\n",
       "      <td>spotify:artist:0A0FS04o6zMoto8OKPsDwY</td>\n",
       "      <td>spotify:track:39hnH8WdPmNT3Q3yzwC9Rg</td>\n",
       "      <td>spotify:album:4nwd6ernojhNIIRifDJoRz</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>92.995</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.3800</td>\n",
       "      <td>Gang Gang!</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17423162</th>\n",
       "      <td>263081</td>\n",
       "      <td>9</td>\n",
       "      <td>Lana Del Rey</td>\n",
       "      <td>Shades Of Cool</td>\n",
       "      <td>Ultraviolence</td>\n",
       "      <td>342093</td>\n",
       "      <td>spotify:artist:00FQb4jTyendYWaN8pK0wa</td>\n",
       "      <td>spotify:track:4VSg5K1hnbmIg4PwRdY6wV</td>\n",
       "      <td>spotify:album:1ORxRsK3MrSLvh7VQTF01F</td>\n",
       "      <td>0.5540</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>137.918</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0878</td>\n",
       "      <td>Piano</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718826</th>\n",
       "      <td>25842</td>\n",
       "      <td>11</td>\n",
       "      <td>Drake</td>\n",
       "      <td>Too Good</td>\n",
       "      <td>Views</td>\n",
       "      <td>263373</td>\n",
       "      <td>spotify:artist:3TVXtAsR1Inumwj472S9r4</td>\n",
       "      <td>spotify:track:11KJSRSgaDxqydKYiD2Jew</td>\n",
       "      <td>spotify:album:3hARKC8cinq3mZLLAEaBh9</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>117.983</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.3920</td>\n",
       "      <td>Another One</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887058</th>\n",
       "      <td>43473</td>\n",
       "      <td>26</td>\n",
       "      <td>Shawn Mendes</td>\n",
       "      <td>Treat You Better</td>\n",
       "      <td>Illuminate</td>\n",
       "      <td>187973</td>\n",
       "      <td>spotify:artist:7n2wHs1TKAczGzO7Dd2rGr</td>\n",
       "      <td>spotify:track:4Hf7WnR761jpxPr5D46Bcd</td>\n",
       "      <td>spotify:album:6FLZDJq4UuwqcaQlI4XCP1</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2460</td>\n",
       "      <td>166.018</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8210</td>\n",
       "      <td>Litness</td>\n",
       "      <td>67</td>\n",
       "      <td>81</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196156</th>\n",
       "      <td>2950</td>\n",
       "      <td>54</td>\n",
       "      <td>Pryda</td>\n",
       "      <td>RYMD - 2010</td>\n",
       "      <td>Inspiration / RYMD (2010)</td>\n",
       "      <td>652969</td>\n",
       "      <td>spotify:artist:37U9sPqTZMd7AKJCWgcvkt</td>\n",
       "      <td>spotify:track:6eoSkeAaET1nGW2w0eyoqR</td>\n",
       "      <td>spotify:album:5wGRS5mPleKVMuCfxOA547</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>125.007</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>SILK</td>\n",
       "      <td>62</td>\n",
       "      <td>67</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Playlistid  Trackid   Artist_Name             Track_Name  \\\n",
       "7417052       111600        0            YG  Why You Always Hatin?   \n",
       "17423162      263081        9  Lana Del Rey         Shades Of Cool   \n",
       "1718826        25842       11         Drake               Too Good   \n",
       "2887058        43473       26  Shawn Mendes       Treat You Better   \n",
       "196156          2950       54         Pryda            RYMD - 2010   \n",
       "\n",
       "                         Album_Name  Track_Duration  \\\n",
       "7417052                 Still Brazy          196600   \n",
       "17423162              Ultraviolence          342093   \n",
       "1718826                       Views          263373   \n",
       "2887058                  Illuminate          187973   \n",
       "196156    Inspiration / RYMD (2010)          652969   \n",
       "\n",
       "                                     Artist_uri  \\\n",
       "7417052   spotify:artist:0A0FS04o6zMoto8OKPsDwY   \n",
       "17423162  spotify:artist:00FQb4jTyendYWaN8pK0wa   \n",
       "1718826   spotify:artist:3TVXtAsR1Inumwj472S9r4   \n",
       "2887058   spotify:artist:7n2wHs1TKAczGzO7Dd2rGr   \n",
       "196156    spotify:artist:37U9sPqTZMd7AKJCWgcvkt   \n",
       "\n",
       "                                     Track_uri  \\\n",
       "7417052   spotify:track:39hnH8WdPmNT3Q3yzwC9Rg   \n",
       "17423162  spotify:track:4VSg5K1hnbmIg4PwRdY6wV   \n",
       "1718826   spotify:track:11KJSRSgaDxqydKYiD2Jew   \n",
       "2887058   spotify:track:4Hf7WnR761jpxPr5D46Bcd   \n",
       "196156    spotify:track:6eoSkeAaET1nGW2w0eyoqR   \n",
       "\n",
       "                                     Album_uri  acousticness   ...   loudness  \\\n",
       "7417052   spotify:album:4nwd6ernojhNIIRifDJoRz        0.0392   ...     -9.326   \n",
       "17423162  spotify:album:1ORxRsK3MrSLvh7VQTF01F        0.5540   ...     -6.863   \n",
       "1718826   spotify:album:3hARKC8cinq3mZLLAEaBh9        0.0573   ...     -7.805   \n",
       "2887058   spotify:album:6FLZDJq4UuwqcaQlI4XCP1        0.1230   ...     -4.105   \n",
       "196156    spotify:album:5wGRS5mPleKVMuCfxOA547        0.0317   ...     -8.652   \n",
       "\n",
       "          mode  speechiness    tempo  time_signature  valence     Playlist  \\\n",
       "7417052    0.0       0.5940   92.995             4.0   0.3800   Gang Gang!   \n",
       "17423162   0.0       0.0302  137.918             3.0   0.0878        Piano   \n",
       "1718826    1.0       0.1170  117.983             4.0   0.3920  Another One   \n",
       "2887058    0.0       0.2460  166.018             4.0   0.8210      Litness   \n",
       "196156     0.0       0.0423  125.007             4.0   0.1550         SILK   \n",
       "\n",
       "          Album  Track  Artist  \n",
       "7417052       6     10       3  \n",
       "17423162      9     17       3  \n",
       "1718826      44     44      40  \n",
       "2887058      67     81      57  \n",
       "196156       62     67      57  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create co-occurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Binary Sparse Matrix\n",
    "co_mat = pd.crosstab(train.Playlistid, train.Track_uri)\n",
    "co_mat = co_mat.clip(upper=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(co_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-159a7458a9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mco_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"co_mat_27k_v2.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(self, path, compression, protocol)\u001b[0m\n\u001b[1;32m   2190\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m         return to_pickle(self, path, compression=compression,\n\u001b[0;32m-> 2192\u001b[0;31m                          protocol=protocol)\n\u001b[0m\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_clipboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexcel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(obj, path, compression, protocol)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_f\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "co_mat.to_pickle(\"co_mat_27k_v2.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4e44ab48e9c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"co_mat_27k.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# pickle.dump(d, open(\"file\", 'w'), protocol=4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(co_mat, open(\"co_mat_27k.pickle\", \"wb\" ), protocol=4)\n",
    "\n",
    "# pickle.dump(d, open(\"file\", 'w'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderSystem():\n",
    "    \"\"\"Represents the scheme for implementing a recommender system.\"\"\"\n",
    "    def __init__(self, training_data, *params):\n",
    "        \"\"\"Initializes the recommender system.\n",
    "\n",
    "        Note that training data has to be provided when instantiating.\n",
    "        Optional parameters are passed to the underlying system.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, *params):\n",
    "        \"\"\"Starts training. Passes optional training parameters to the system.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def score(self, user_id, item_id):\n",
    "        \"\"\"Returns a single score for a user-item pair.\n",
    "\n",
    "        If no prediction for the given pair can be made, an exception should be raised.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ALSRecommenderSystem(RecommenderSystem):\n",
    "    \"\"\"Provides a biased ALS-based implementation of an implicit recommender system.\"\"\"\n",
    "    def __init__(self, training_data, biased, latent_dimension, log_dir=None, confidence=20):\n",
    "        \"\"\"Initializes the recommender system.\n",
    "\n",
    "        Keyword arguments:\n",
    "        training_data: Data to train on.\n",
    "        biased: Whether to include user- and item-related biases in the model.\n",
    "        latent_dimension: Dimension of the latent space.\n",
    "        log_dir: Optional pointer to directory storing logging information.\n",
    "        confidence: Confidence value that should be assigned to pairs where interaction\n",
    "                    was present. Since the data includes single interactions only, simply\n",
    "                    assigining 1 for non-interactions and this value otherwise suffices.\n",
    "                    Should be greater than 1.\n",
    "        \"\"\"\n",
    "        self.biased = biased\n",
    "        self.confidence = confidence\n",
    "        self.latent_dimension = latent_dimension\n",
    "        self.U = None\n",
    "        self.V = None\n",
    "        self.log_dir = log_dir\n",
    "        self.C_users, self.P_users, self.C_items, self.P_items, self.mapping_users, self.mapping_items = self._build_matrices(training_data, confidence)\n",
    "        self.user_dim, self.item_dim = self.P_users.shape\n",
    "\n",
    "    def _build_matrices(self, activity, confidence):\n",
    "        \"\"\"Build the initial matrices.\"\"\"\n",
    "        distinct_users = len(set(activity['user']))\n",
    "        distinct_items = len(set(activity['items']))\n",
    "        C_users = np.ones(shape=(distinct_users, distinct_items))\n",
    "        P_users = np.zeros(shape=(distinct_users, distinct_items))\n",
    "        C_items = np.ones(shape=(distinct_items, distinct_users))\n",
    "        P_items = np.zeros(shape=(distinct_items, distinct_users))\n",
    "\n",
    "        mapping_users = {}\n",
    "        mapping_items = {}\n",
    "        user_ct = 0\n",
    "        items_ct = 0\n",
    "\n",
    "        for index, row in activity.iterrows():\n",
    "            user, items = row\n",
    "            if not user in mapping_users:\n",
    "                mapping_users[user] = user_ct\n",
    "                user_ct += 1\n",
    "            if not items in mapping_items:\n",
    "                mapping_items[items] = items_ct\n",
    "                items_ct += 1\n",
    "            user_index, items_index = mapping_users[user], mapping_items[items]\n",
    "            C_users[user_index, items_index] = confidence\n",
    "            P_users[user_index, items_index] = 1\n",
    "            C_items[items_index, user_index] = confidence\n",
    "            P_items[items_index, user_index] = 1\n",
    "        return C_users, P_users, C_items, P_items, mapping_users, mapping_items\n",
    "\n",
    "    def save(self, directory):\n",
    "        \"\"\"Saves current matrices to the given directory.\"\"\"\n",
    "        np.save(os.path.join(directory, 'U.npy'), self.U)\n",
    "        np.save(os.path.join(directory, 'V.npy'), self.V)\n",
    "        #np.save(os.path.join(directory, 'training_data.npy'), self.training_data)\n",
    "        np.save(os.path.join(directory, 'params.npy'), np.array([self.confidence]))\n",
    "        if self.biased:\n",
    "            np.save(os.path.join(directory, 'user_biases.npy'), self.user_biases)\n",
    "            np.save(os.path.join(directory, 'item_biases.npy'), self.item_biases)\n",
    "\n",
    "    def load(self, directory):\n",
    "        \"\"\"Loads matrices from the given directory.\"\"\"\n",
    "        self.U = np.load(os.path.join(directory, 'U.npy'))\n",
    "        self.V = np.load(os.path.join(directory, 'V.npy'))\n",
    "        self.training_data = np.load(os.path.join(directory, 'training_data.npy'))\n",
    "        self.confidence = np.load(os.path.join(directory, 'params.npy')).flatten()\n",
    "        if self.biased:\n",
    "            self.user_biases = np.load(os.path.join(directory, 'user_biases.npy'))\n",
    "            self.item_biases = np.load(os.path.join(directory, 'item_biases.npy'))\n",
    "\n",
    "        self.C_users, self.P_users, self.C_items, self.P_items, self.mapping_users, self.mapping_items = self._build_matrices(self.training_data, self.confidence)\n",
    "        self.user_dim, self.item_dim = self.P_users.shape\n",
    "\n",
    "    def _single_step(self, lbd):\n",
    "        \"\"\"Executes a single optimization step using (biased) ALS, with lbd as regularization factor.\"\"\"\n",
    "        C_users, P_users, C_items, P_items, mapping_users, mapping_items = self.C_users, self.P_users, self.C_items, self.P_items, self.mapping_users, self.mapping_items\n",
    "        biased = self.biased\n",
    "\n",
    "        # Update U.\n",
    "        if biased: # Expand matrices to account for biases.\n",
    "            U_exp = np.hstack((self.user_biases.reshape(-1,1), self.U))\n",
    "            V_exp = np.hstack((np.ones_like(self.item_biases).reshape(-1,1), self.V))\n",
    "            kdim = self.latent_dimension + 1\n",
    "        else: # We work with copies here to make it safer to abort within updates.\n",
    "            U_exp = self.U.copy()\n",
    "            V_exp = self.V.copy()\n",
    "            kdim = self.latent_dimension\n",
    "        Vt = np.dot(np.transpose(V_exp), V_exp)\n",
    "        for user_index in tqdm(range(self.user_dim)):\n",
    "            C = np.diag(C_users[user_index])\n",
    "            d = np.dot(C, P_users[user_index] - (0 if not biased else self.item_biases))\n",
    "            val = np.dot(np.linalg.inv(Vt + np.dot(np.dot(V_exp.T, C - np.eye(self.item_dim)), V_exp) + lbd*np.eye(kdim)), np.transpose(V_exp))\n",
    "            U_exp[user_index] = np.dot(val, d)\n",
    "        if biased:\n",
    "            self.user_biases = U_exp[:,0]\n",
    "            self.U = U_exp[:,1:]\n",
    "        else:\n",
    "            self.U = U_exp\n",
    "\n",
    "        # Update V.\n",
    "        if biased:\n",
    "            U_exp = np.hstack((np.ones_like(self.user_biases).reshape(-1,1), self.U))\n",
    "            V_exp = np.hstack((self.item_biases.reshape(-1,1), self.V))\n",
    "        else: # We work with copies here to make it safer to abort within updates.\n",
    "            U_exp = self.U.copy()\n",
    "            V_exp = self.V.copy()\n",
    "\n",
    "        Ut = np.dot(np.transpose(U_exp), U_exp)\n",
    "        for item_index in tqdm(range(self.item_dim)):\n",
    "            C = np.diag(C_items[item_index])\n",
    "            d = np.dot(C, P_items[item_index] - (0 if not biased else self.user_biases))\n",
    "            val = np.dot(np.linalg.inv(Ut + np.dot(np.dot(U_exp.T, C-np.eye(self.user_dim)), U_exp) + lbd*np.eye(kdim)), np.transpose(U_exp))\n",
    "            V_exp[item_index] = np.dot(val, d)\n",
    "        if biased:\n",
    "            self.item_biases = V_exp[:, 0]\n",
    "            self.V = V_exp[:,1:]\n",
    "        else:\n",
    "            self.V = V_exp\n",
    "\n",
    "    def compute_loss(self, lbd):\n",
    "        \"\"\"Computes loss value on the training data.\n",
    "\n",
    "        Returns a tuple of total loss and prediction loss (excluding regularization loss).\n",
    "        \"\"\"\n",
    "        C_users, P_users, C_items, P_items, mapping_users, mapping_items = self.C_users, self.P_users, self.C_items, self.P_items, self.mapping_users, self.mapping_items\n",
    "        main_loss = 0\n",
    "        # Main loss term.\n",
    "        for user_index in range(self.user_dim):\n",
    "            for item_index in range(self.item_dim):\n",
    "                pred = np.dot(self.U[user_index].T, self.V[item_index])\n",
    "                if self.biased:\n",
    "                    pred += self.user_biases[user_index] + self.item_biases[item_index]\n",
    "                loss = self.C_users[user_index, item_index] * (P_users[user_index, item_index]-pred)**2\n",
    "                main_loss += loss\n",
    "\n",
    "        # Regularization term.\n",
    "        reg_loss = 0\n",
    "        if lbd > 0:\n",
    "            for user_index in range(self.user_dim):\n",
    "                reg_loss += np.sum(self.U[user_index]**2) + (0 if not self.biased else self.user_biases[user_index]**2)\n",
    "            for item_index in range(self.item_dim):\n",
    "                reg_loss += np.sum(self.V[item_index]**2) + (0 if not self.biased else self.item_biases[item_index]**2)\n",
    "            reg_loss *= lbd\n",
    "        return main_loss + reg_loss, main_loss\n",
    "\n",
    "    def train(self, lbd, iterations=20, verbose=True):\n",
    "        \"\"\"\n",
    "        Trains the recommendation system.\n",
    "\n",
    "        Keyword arguments:\n",
    "        lbd: Regularization factor.\n",
    "        iterations: Number of iterations to run ALS.\n",
    "        verbose: Whether to plot and output training loss.\n",
    "        \"\"\"\n",
    "        if self.U is None or self.V is None:\n",
    "            self.U = np.random.normal(size=(self.user_dim, self.latent_dimension))\n",
    "            self.V = np.random.normal(size=(self.item_dim, self.latent_dimension))\n",
    "            self.user_biases = np.zeros(self.user_dim)\n",
    "            self.item_biases = np.zeros(self.item_dim)\n",
    "            self.history_losses = []\n",
    "            self.history_main_losses = []\n",
    "            self.history_avg_score = []\n",
    "            self.history_avg_rank = []\n",
    "\n",
    "        it = 0\n",
    "        while(it < iterations):\n",
    "            self._single_step(lbd)\n",
    "            loss, main_loss = self.compute_loss(lbd)\n",
    "            self.history_losses.append(loss)\n",
    "            self.history_main_losses.append(main_loss)\n",
    "\n",
    "            if verbose:\n",
    "                clear_output(wait=True)\n",
    "                print('LOSS:', loss, 'MAIN LOSS:', main_loss)\n",
    "\n",
    "                plt.figure(figsize=(5,5))\n",
    "                plt.title('training loss (lower is better)')\n",
    "                plt.plot(range(len(self.history_losses)), self.history_losses)\n",
    "                plt.plot(range(len(self.history_main_losses)), self.history_main_losses, color='orange')\n",
    "                plt.plot(range(len(self.history_main_losses)), np.array(self.history_losses) - np.array(self.history_main_losses), color='green')\n",
    "                plt.legend(['total loss', 'data loss', 'regularizing loss'])\n",
    "                if self.log_dir is not None:\n",
    "                    plt.savefig(os.path.join(self.log_dir, 'log.png'), bbox_inches='tight', format='png')\n",
    "                plt.show()\n",
    "            it += 1\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the recommendation system's internal state.\"\"\"\n",
    "        self.U = None\n",
    "        self.V = None\n",
    "\n",
    "    def score(self, user_id, items_id):\n",
    "        \"\"\"Returns the scoring of item_id for user_id.\"\"\"\n",
    "        if self.U is None or self.V is None:\n",
    "            raise ValueError('system has to be trained first')\n",
    "        if user_id not in self.mapping_users:\n",
    "            raise ValueError('user unknown')\n",
    "        if items_id not in self.mapping_items:\n",
    "            raise ValueError('item unknown')\n",
    "\n",
    "        user_index = self.mapping_users[user_id]\n",
    "        items_index = self.mapping_items[items_id]\n",
    "        pred = np.dot(self.U[user_index], self.V[items_index])\n",
    "        if self.biased: # Include applicable biases.\n",
    "            pred += self.user_biases[user_index] + self.item_biases[items_index]\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:0UaMYEvWZi0ZqiDOoHU3YI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:0WqIKmW4BTrj3eJFmnCKMv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:0XUfyU2QviPAs6bxSpXYG4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:0uqPG793dkDDN7sCUJJIVC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:12qZHAeOyTf93YAWvGDTat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user                                 items\n",
       "0    0  spotify:track:0UaMYEvWZi0ZqiDOoHU3YI\n",
       "1    0  spotify:track:0WqIKmW4BTrj3eJFmnCKMv\n",
       "2    0  spotify:track:0XUfyU2QviPAs6bxSpXYG4\n",
       "3    0  spotify:track:0uqPG793dkDDN7sCUJJIVC\n",
       "4    0  spotify:track:12qZHAeOyTf93YAWvGDTat"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for i, row in co_mat.iterrows():\n",
    "    for track in row[row == 1].index.values:\n",
    "        res.append((i, track))\n",
    "res = pd.DataFrame(np.array(res), columns=['user', 'items'])\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(res, open(\"res_27k.pickle\", \"wb\" ), protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = ALSRecommenderSystem(res, True, latent_dimension=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.train(0.01, iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_similar_songs_playlist(model, orig_df, target_playlist_id, cand_list_size):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    model: the recommendation system that was trained on the training set with latent factors\n",
    "    orig_df: original df with tracks as rows, but with playlistid and other features (e.g., train)\n",
    "    target_playlist_id: id of the target playlist\n",
    "    target_playlist_inx: index of playlist in the training set\n",
    "    cand_list_size: candidate list of songs to recommend size (= test-set size * 15)\n",
    "    \n",
    "    Output:\n",
    "    k_song_to_recommend: the most similar tracks per track\n",
    "    \"\"\"\n",
    "    target_track_inx = np.where(train[\"Playlistid\"] == target_playlist_id)[0] # index of tracks in training playlist of target playlist\n",
    "    score_allsongs = list(map(lambda x: model.score(str(target_playlist_id), x), orig_df[\"Track_uri\"]))\n",
    "    rec_inx = np.argsort(score_allsongs)[::-1]\n",
    "    \n",
    "    cand_list = orig_df.iloc[rec_inx]['Track_uri']\n",
    "    unique_cand_list = cand_list.drop_duplicates()#list(set(cand_list)) # drop duplciated tracks\n",
    "    \n",
    "    tracks_in_target_playlist = orig_df.loc[orig_df[\"Playlistid\"] == target_playlist_id, \"Track_uri\"]\n",
    "    \n",
    "    cand_list2 = unique_cand_list.loc[~unique_cand_list.isin(tracks_in_target_playlist)] # remove songs that are in the \n",
    "    cand_list3 = cand_list2[:cand_list_size]\n",
    "    return list(cand_list3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nholdout(playlist_id, df):\n",
    "    '''Pass in a playlist id to get number of songs held out in val/test set'''\n",
    "    return len(df[df.Playlistid == playlist_id].Track_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_precision(prediction, val_set):\n",
    "    # prediction should be a list of predictions\n",
    "    # val_set should be pandas Series of ground truths\n",
    "    score = np.sum(val_set.isin(prediction))/val_set.shape[0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NDCG Code Source: https://gist.github.com/bwhite/3726239\n",
    "def dcg_at_k(r, k, method=0):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rps = []\n",
    "ndcgs = []\n",
    "for pid in co_mat.index:\n",
    "    ps = als_similar_songs_playlist(rs, train, pid, nholdout(pid, train)*15)\n",
    "    vs = test[test.Playlistid == pid].Track_uri # ground truth\n",
    "    rps.append(r_precision(ps, vs))\n",
    "    \n",
    "    r = np.zeros(len(ps))\n",
    "    for i, p in enumerate(ps):\n",
    "        if np.any(vs.isin([p])):\n",
    "            r[i] = 1\n",
    "    ndcgs.append(ndcg_at_k(r, len(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. R-Precision:  0.23969273359366242\n",
      "Avg. NDCG:  0.1504029917610824\n",
      "Total Sum:  0.1950478626773724\n"
     ]
    }
   ],
   "source": [
    "avg_rp = np.mean(rps)\n",
    "avg_ndcg = np.mean(ndcgs)\n",
    "print('Avg. R-Precision: ', avg_rp)\n",
    "print('Avg. NDCG: ', avg_ndcg)\n",
    "print('Total Sum: ', np.mean([avg_rp, avg_ndcg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
